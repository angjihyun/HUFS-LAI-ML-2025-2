# HUFS (Language & AI 융합학부) - 기계학습의이해 (2025-2)

# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 96.98%
- 훈련 시간: 48.6초 (0.8분)

## 실험 결과
### 실험 1: 러닝레이트 변경
- baseline: lr = 1e-3 (0.001)  
  -> 최종 테스트 정확도: 96.95% 
- 변경사항: 학습률을 작은 값부터 큰 값으로 바꿔가며 실험했습니다.8가지 learning rate(1e-5 ~ 3e-2)에서 학습을 진행하여 너무 크거나 작은 학습률이 학습 과정에 어떤 영향을 주는지 확인했습니다.

- 결과:=== 러닝레이트 실험 시작 ===

Learning Rate 1e-05 -> Test Accuracy: 84.44%
Learning Rate 3e-05 -> Test Accuracy: 89.92%
Learning Rate 1e-04 -> Test Accuracy: 92.68%
Learning Rate 3e-04 -> Test Accuracy: 95.31%
Learning Rate 1e-03 -> Test Accuracy: 97.08%
Learning Rate 3e-03 -> Test Accuracy: 96.90%
Learning Rate 1e-02 -> Test Accuracy: 95.84%
Learning Rate 3e-02 -> Test Accuracy: 92.88%

=== 실험 종료 ===
- 분석: 실험1 에서는 학습률이 너무 작을 때(1e-5~1e-4)는 학습이 잘 진행되지 않아 정확도가 낮게 나왔습니다. 반대로 1e-2 이상으로 커지면 손실이 불안정하게 변하면서 정확도가 다시 떨어졌습니다.
 1e-3 부근에서 가장 높은 정확도(97.08%)가 나왔고 학습률이 너무 작으면 최적점에 도달하기 어렵고 너무 크면 최소점을 지나치는 문제를 확인할수 있었습니다. 
 러닝레이트가 너무 크고 작음을 에포크로 보완할수 있을것 같다는 생각에 실험2를 진행하게 됐습니다.  (러닝레이트의 범위와 실험을 위한 각 러닝레이트를 정하는데에 제미나이를 참고하였습니다.)

### 실험 2:  러닝레이트와 에포크 조합 비교
- baseline: lr=1e-3, epoch=3
 -> 최종 테스트 정확도: 96.95% 
- 변경사항:learning rate를 [1e-5, 1e-3, 3e-2]로, epoch을 [3, 5, 10]으로 설정하여 조합별 실험을 진행하였습니다.
 작은 learning rate에서는 더 많은 epoch이 적절하고 큰 learning rate에서는 적은 epoch이 적절할것이라는 가설을 세우고 비교하였습니다.
- 결과:=== 러닝레이트랑 에포크 비교 실험 시작 ===

Learning Rate 1e-05, Epochs 3 -> Test Accuracy: 85.12%
Learning Rate 1e-05, Epochs 5 -> Test Accuracy: 87.95%
Learning Rate 1e-05, Epochs 10 -> Test Accuracy: 90.37%
Learning Rate 1e-03, Epochs 3 -> Test Accuracy: 96.94%
Learning Rate 1e-03, Epochs 5 -> Test Accuracy: 97.49%
Learning Rate 1e-03, Epochs 10 -> Test Accuracy: 97.76%
Learning Rate 3e-02, Epochs 3 -> Test Accuracy: 91.29%
Learning Rate 3e-02, Epochs 5 -> Test Accuracy: 90.38%
Learning Rate 3e-02, Epochs 10 -> Test Accuracy: 91.50%

=== 실험 종료 ===

- 분석:작은 learning rate(1e-5)에서는 epoch을 늘릴수록 정확도가 꾸준히 증가하였습니다. 이는 한 번의 가중치 업데이트 폭이 작기 때문에 많은 반복을 통해 손실함수 값을 줄여야 하기 때문이고, 훈련시간도 길어집니다.
반면 큰 learning rate(3e-2)에서는 처음 작은 epoch만으로도 높은 정확도를 보여주나 훈련데이터에는 잘맞지만 테스트데이터에는 일반화 성능이 떨어지는 과적합 경향을 보였습니다.
결과적으로 작은 학습률일수록 더 많은 epoch이 필요하고 큰 학습률일수록 빠르게 학습되지만 안정성은 떨어진다는 경향을 확인할 수 있었습니다.
따라서 두 하이퍼파라미터는 서로 균형을 이루어야 하며 실험 2에서는 학습률 1e-3 수준에서 적절한 epoch을 설정하는 것이 가장 안정적인 결과를 얻을수 있음을 확인할수 있었습니다.




### 실험 3: 은닉층 크기와 개수 조정
- baseline: hidden layer = [100]
 -> 최종 테스트 정확도: 96.98%
- 변경사항: 은닉층의 개수를 단계적으로 늘려가며 실험을 진행했습니다. 모델 구조는 [100], [100, 50], [100, 50, 25], [100, 50, 25, 12] 형태로 설정했습니다.
- 결과:=== 은닉층 개수별 성능 비교 실험 시작 ===

구조 [100] → Test Accuracy: 96.80%
구조 [100, 50] → Test Accuracy: 96.92%
구조 [100, 50, 25] → Test Accuracy: 97.17%
구조 [100, 50, 25, 12] → Test Accuracy: 96.77%

=== 실험 종료 ===
- 분석:은닉층의 개수를 늘릴수록 모델이 더 복잡해지며 데이터의 복잡한 패턴과 특징들을 더 세밀하게 학습할 수 있습니다. 실험 3에서 실제로 3층 구조 [100, 50, 25]에서 정확도가 가장 높게 나왔습니다.
그러나 4층 구조 [100, 50, 25, 12]에서는 정확도가 약간 감소했는데 이는 모델 복잡도가 증가하면서 과적합이 발생했기 때문으로 판단했습니다. 
즉 은닉층이 많다고 해서 항상 성능이 좋아지는 것은 아니며 모델에 적절한 깊이와 크기의 구조를 선택해야 한다는 결과를 확인했습니다.(은닉층을 넣고싶은데 위치와 방법을 몰라 어디에 어떻게 넣어야할지 제미나이의 도움을 받았습니다.)




### 실험 4: confusion matrix 생성
- 목적: 단순히 정확도 수치만으로는 어떤 숫자에서 오분류가 일어나는지 확인하기 어렵기 때문에 confusion matrix를 통해 모델이 어떤 숫자를 자주 혼동하는지를 시각적으로 분석했습니다.
- 설정: 지금까지의 실험(1~3)을 통해 가장 높은 정확도를 보였던 모델 Learning Rate = 1e-3, 은닉층 구조 = [100, 50, 25] 이 설정을 기본 모델(best model) 로 두고 confusion matrix를 생성했습니다.
(정확도 향상이 아닌 모델의 예측 분포를 분석하기 위해 에포크는 3 그대로 두었습니다)
- 결과: 대부분의 숫자 클래스에서 정답위치에 높은 값이 분포하고 있으며 이는 모델이 전반적으로 각 숫자를 정확히 구분하고 있음을 알수있습니다.
특히 1, 7과 같은 형태가 단순한 숫자는 오분류가 거의 발생하지 않았습니다. 5, 6, 8, 9 등의 필기체 형태가 유사한 숫자들 사이에선 일부 혼동이 나타났습니다. 
confusion matrix로 확인한 결과 모델의 성능은 충분하지만, 입력인 필기체의 유사성으로(데이터간의 유사성) 생긴 한계라고 해석할수 있었습니다.

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 전체 실험 중에서는 학습률 1e-3과 에포크 10의 조합이 가장 안정적이고 높은 정확도(97.76%)를 보였습니다.
- 관찰된 패턴: 학습률이 너무 작으면 학습 속도가 느려지고 정확도가 낮게 유지되었으며 너무 크면 손실이 불안정하게 진동하거나 발산하는 경향을 보였습니다.
작은 학습률에서는 에포크를 늘릴수록 성능이 향상되었지만 큰 학습률에서는 과적합으로 인해 테스트 정확도가 감소했습니다.
은닉층이 많을수록 복잡한 패턴을 학습할 수 있었지만 3층을 넘어가면 과적합으로 인해 오히려 성능이 떨어지는 양상을 보였습니다.
- 추가 개선 아이디어:dropout을 도입하여 은닉층이 많은 모델의 과적합을 완화할 수 있을것 같습니다. dropout은 학습중에 일부뉴런의 출력을 확률적으로 조정하기 때문에 특정 노드에 의존하지 않게 만들어 일반화 성능을 향상시킬수 있기 때문입니다.
adaptive learning rate을 활용하여 '학습률이 너무 크면 발산, 너무 작으면 수렴 지연' 이라는 문제를 해결할수 있을것 같습니다. adaptive learning rate는 학습 초반에는 큰 학습률로 빠르게 손실을 줄이고 중간부터 자동으로 학습률을 낮춰 더 정밀하고 효율적인 학습이 가능하기 때문입니다.